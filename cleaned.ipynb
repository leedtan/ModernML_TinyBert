{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data.py', 'pretrained_model.py', 'transformer.py', 'utils.py']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "[f for f in os.listdir() if '.py' in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from utils import *\n",
    "from pretrained_model import *\n",
    "from transformer import *\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset('enwiki-latest-pages-articles_preprocessed.txt')\n",
    "\n",
    "#Wrap it around a dataloader\n",
    "dataloader = DataLoader(dataset, batch_size = 2, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import leaky_relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trf =  Transformer(d_model = 100, nhead = 2, num_encoder_layers = 3, \n",
    "                   dim_feedforward = 100, dropout = .1, activation = 'lrelu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 100])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trf(torch.rand((10, 32, 100)), src_key_padding_mask=torch.ones((32, 10))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = PretrainedModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 100])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(torch.rand((10, 32, 100)), 1, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = Embedder(5, 4)\n",
    "# PE(emb(torch.tensor([[1, 2, 3]]))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "VOCAB_SIZE = vocab_size = tok.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyBert(nn.Module):\n",
    "    def __init__(self, vocab_size = VOCAB_SIZE, emb_size=144, nhead = 12, num_encoder_layers = 6):\n",
    "        super(TinyBert, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.model = Transformer(\n",
    "            d_model = emb_size, nhead = nhead, num_encoder_layers = num_encoder_layers, \n",
    "            dim_feedforward = emb_size, dropout = .1, activation = 'lrelu')\n",
    "        self.embedder = Embedder(vocab_size, emb_size)\n",
    "        self.PE = PositionalEncoding(emb_size)\n",
    "    def forward(self, src, mask=None):\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(src, dtype = float)\n",
    "        #reshaping cus trf module is stupid\n",
    "        self.mask = mask\n",
    "        self.emb_raw = emb_raw = self.embedder(src)\n",
    "        self.emb = emb = self.PE(emb_raw)\n",
    "        self.emb_transposed = emb_transposed = torch.transpose(emb, 1, 0)\n",
    "        self.trf_output = trf_output = self.model(emb_transposed, src_key_padding_mask=mask)\n",
    "        return trf_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TinyBert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tb(torch.tensor([[1, 2, 3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "??np.random.choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "import torch.optim as optim\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.pretrained_model = PretrainedModel()\n",
    "        self.tinybert = TinyBert()\n",
    "#         self.tinybert = Transformer(d_model = 100, nhead = 2, num_encoder_layers = 3, \n",
    "#                    dim_feedforward = 100, dropout = .1, activation = 'lrelu')\n",
    "        self.tokenizer = self.pretrained_model.tokenizer\n",
    "        self.y = []\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=0.01)\n",
    "\n",
    "    def forward(self, text):\n",
    "        if isinstance(text[0], list):\n",
    "            return self.forward_sentence(text)\n",
    "        elif isinstance(text[0], str):\n",
    "            return self.forward_maskLM(text)\n",
    "        else:\n",
    "            raise ValueError('wtf is this text?' + text + type(text[0]))\n",
    "            \n",
    "    def preprocess_LM(self, text):\n",
    "        self.y = []\n",
    "        sentences = [build_sentence_list(\n",
    "            'CLS', [self.tokenizer.tokenize(line)]) for line in text]\n",
    "        \n",
    "        lengths = [len(sentence) - 2 for sentence in sentences]\n",
    "        mask_idxes = [np.random.choice(length, size=math.ceil(length/7), replace=False) for length in lengths]\n",
    "        \n",
    "        masks = [np.ones(length + 2) for length in lengths]\n",
    "        for mask_idxes, mask, sentence in zip(mask_idxes, masks, sentences):\n",
    "            self.y.append([])\n",
    "            for mask_idx in mask_idxes:\n",
    "                mask[mask_idx + 1] = 0\n",
    "                self.y[-1].append(sentence[mask_idx + 1])\n",
    "                sentence[mask_idx + 1] = '[MASK]'\n",
    "        self.attention_mask = attention_mask = to_cuda(torch.tensor(pad_sequences(masks, padding='post')))\n",
    "        self.tokenized_text = tokenized_text = to_cuda(torch.tensor(pad_sequences([\n",
    "            self.tokenizer.convert_tokens_to_ids(sentence) for sentence in sentences]).tolist()))\n",
    "        return tokenized_text, attention_mask\n",
    "    def forward_maskLM(self, text):\n",
    "        tokenized_text, attention_mask = self.preprocess_LM(text)\n",
    "        self.pretrained_hidden, self.pretrained_attn = pretrained_hidden, pretrained_attn = self.pretrained_model(\n",
    "            tokenized_text = tokenized_text, attention_mask = attention_mask)\n",
    "        self.tb_out = tb_out = self.tinybert(tokenized_text, mask=attention_mask)\n",
    "        self.tb_out_masked = tb_out_masked = tb_out * attention_mask.transpose(1, 0).unsqueeze(-1)\n",
    "        return tokenized_text, attention_mask, tb_out_masked, pretrained_hidden, pretrained_attn\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mdl = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset('enwiki-latest-pages-articles_preprocessed.txt')\n",
    "\n",
    "#Wrap it around a dataloader\n",
    "dataloader = DataLoader(dataset, batch_size = 8, num_workers = 0, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 ['To axiomatize a system of knowledge is to show that its claims can be derived from a small, well-understood set of sentences (the axioms), and there may be multiple ways to axiomatize a given mathematical domain.', 'The subsidized air company Air Tahiti Nui brings tourists from France, Los Angeles, Japan and China.', 'Uzzayanâ\\x80\\x99s cult in particular was widespread in south Arabia, and in Qataban, she was invoked as a guardian of the final royal palace.', 'Recommended dosing interval is 4â\\x80\\x936 hours.', 'In the aftermath of the incident, Steve and Sam plan to keep what happened at Pleasant Hill under wraps for the time being.', 'This area is the traditional homeland of the Tlingit, and home of a historic settling of Haida as well as a modern settlement of Tsimshian.', 'This combination of lifelong military experience and monetary incentives resulted in a cohesive, well-disciplined military.', 'Many dynasties have their own specific adaptation of Nusach Sefard; some, such as the versions of the Belzer, Bobover, and Dushinsky Hasidim, are closer to Nusach Ashkenaz, while others, such as the Munkacz version, are closer to the old Lurianic.']\n"
     ]
    }
   ],
   "source": [
    "itr = 0\n",
    "for text in dataloader:\n",
    "    print(len(text), text)\n",
    "    itr += 1\n",
    "    if itr > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len([v.shape for v in mdl(['hi', 'strawberries'])[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, hid, attn = mdl(['hi', 'strawberries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 12)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hid), len(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 12, 4, 4]),\n",
       " torch.Size([2, 12, 4, 4]),\n",
       " torch.Size([2, 12, 4, 4]),\n",
       " torch.Size([2, 12, 4, 4]),\n",
       " torch.Size([2, 12, 4, 4]),\n",
       " torch.Size([2, 12, 4, 4]),\n",
       " torch.Size([2, 12, 4, 4]),\n",
       " torch.Size([2, 12, 4, 4]),\n",
       " torch.Size([2, 12, 4, 4]),\n",
       " torch.Size([2, 12, 4, 4]),\n",
       " torch.Size([2, 12, 4, 4]),\n",
       " torch.Size([2, 12, 4, 4])]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a.shape for a in attn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 4, 768]),\n",
       " torch.Size([2, 4, 768]),\n",
       " torch.Size([2, 4, 768]),\n",
       " torch.Size([2, 4, 768]),\n",
       " torch.Size([2, 4, 768]),\n",
       " torch.Size([2, 4, 768]),\n",
       " torch.Size([2, 4, 768]),\n",
       " torch.Size([2, 4, 768]),\n",
       " torch.Size([2, 4, 768]),\n",
       " torch.Size([2, 4, 768]),\n",
       " torch.Size([2, 4, 768]),\n",
       " torch.Size([2, 4, 768]),\n",
       " torch.Size([2, 4, 768])]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[h.shape for h in hid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text, attention_mask, tb_out_masked, pretrained_hidden, pretrained_attn = mdl(['hi there how are you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained.model.embeddings(tokenized_text).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,\n",
       " [torch.Size([1, 12, 7, 7]),\n",
       "  torch.Size([1, 12, 7, 7]),\n",
       "  torch.Size([1, 12, 7, 7]),\n",
       "  torch.Size([1, 12, 7, 7]),\n",
       "  torch.Size([1, 12, 7, 7]),\n",
       "  torch.Size([1, 12, 7, 7]),\n",
       "  torch.Size([1, 12, 7, 7]),\n",
       "  torch.Size([1, 12, 7, 7]),\n",
       "  torch.Size([1, 12, 7, 7]),\n",
       "  torch.Size([1, 12, 7, 7]),\n",
       "  torch.Size([1, 12, 7, 7]),\n",
       "  torch.Size([1, 12, 7, 7])])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pretrained_attn), [h.shape for h in pretrained_attn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13,\n",
       " [torch.Size([1, 7, 768]),\n",
       "  torch.Size([1, 7, 768]),\n",
       "  torch.Size([1, 7, 768]),\n",
       "  torch.Size([1, 7, 768]),\n",
       "  torch.Size([1, 7, 768]),\n",
       "  torch.Size([1, 7, 768]),\n",
       "  torch.Size([1, 7, 768]),\n",
       "  torch.Size([1, 7, 768]),\n",
       "  torch.Size([1, 7, 768]),\n",
       "  torch.Size([1, 7, 768]),\n",
       "  torch.Size([1, 7, 768]),\n",
       "  torch.Size([1, 7, 768]),\n",
       "  torch.Size([1, 7, 768])])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pretrained_hidden), [h.shape for h in pretrained_hidden]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_unimplemented',\n",
       " '_get_name',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'half',\n",
       " 'load_state_dict',\n",
       " 'model',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'tokenizer',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
